# Metadata & Observability Reference

The Allos SDK includes a sophisticated observability layer that normalizes data from different LLM providers into a single, consistent **Metadata Schema**.

Whether you are using OpenAI, Anthropic, or a local Ollama model, the usage metrics, latency timings, and cost estimates will always be accessible via the same standardized object.

## Accessing Metadata

There are two primary ways to access metadata, depending on how you use the SDK.

### 1. Via the Agent (Aggregated)

When using the `Agent` class, metadata is aggregated across the entire "Reason-Act" loop. This gives you the total cost and token usage for a multi-turn task.

```python
from allos import Agent

agent = Agent(...)
response = agent.run("Find the file 'data.csv' and summarize it.")

# Access metadata for the last completed run
meta = agent.last_run_metadata

print(f"Total Cost: ${meta.usage.estimated_cost.total_usd}")
print(f"Total Duration: {meta.latency.total_duration_ms}ms")
print(f"Total Turns: {meta.turns.total_turns}")
```

### 2. Via the Provider (Per-Request)

When using a `Provider` directly (or inspecting individual steps in the Agent's code), you get metadata for a specific API call.

```python
response = provider.chat(messages)
print(f"Tokens used in this call: {response.metadata.usage.total_tokens}")
```

---

## The Metadata Schema

The metadata object is a strict Pydantic model (`allos.providers.metadata.Metadata`). Below is the hierarchy of its components.

### Top-Level Fields

| Field | Type | Description |
| :--- | :--- | :--- |
| `request_id` | `str` | A unique identifier for the request (UUID). |
| `timestamp` | `str` | ISO 8601 timestamp of when the metadata was finalized. |
| `status` | `str` | The outcome of the request (e.g., `"success"`, `"error"`). |
| `model` | `ModelInfo` | Details about the model used. |
| `usage` | `Usage` | Token counts and cost estimates. |
| `latency` | `Latency` | Timing metrics. |
| `tools` | `ToolInfo` | Details on tool availability and execution. |
| `turns` | `TurnsInfo` | (Agent only) History of the agentic loop. |
| `quality_signals` | `QualitySignals` | Indicators of response quality (e.g., stop reasons). |

### `ModelInfo`
Details about the model configuration.
- `provider`: `str` (e.g., `"openai"`, `"anthropic"`)
- `model_id`: `str` (e.g., `"gpt-4o"`)
- `configuration`: `ModelConfiguration`
    - `temperature`: `float`
    - `max_output_tokens`: `int`

### `Usage` & `EstimatedCost`
The core metrics for consumption.
- `input_tokens`: `int` - Tokens sent to the model.
- `output_tokens`: `int` - Tokens generated by the model.
- `total_tokens`: `int` - Sum of input and output.
- `estimated_cost`: `EstimatedCost`
    - `input_cost_usd`: `float`
    - `output_cost_usd`: `float`
    - `total_usd`: `float` - The estimated total cost in US Dollars.

> [!NOTE] Pricing Source
> Costs are calculated based on a static internal pricing table (`_STATIC_PRICING`) within the SDK. This is an **estimate** and may not reflect real-time price changes, volume discounts, or enterprise agreements.

### `Latency`
- `total_duration_ms`: `int` - Total wall-clock time for the request (or agent run) in milliseconds.
- `time_to_first_token_ms`: `Optional[int]` - (Streaming only) Time elapsed before the first token was received.

### `TurnsInfo` (Agent Only)
When running an `Agent`, this object tracks the "Reason-Act" loop.
- `total_turns`: `int` - How many times the agent called the LLM.
- `turn_history`: `List[TurnLog]` - A chronological log of each step.
    - `turn_number`: `int`
    - `model_used`: `str`
    - `content_type`: `str` (e.g., `"text_response"` or `"tool_calls"`)
    - `tokens_used`: `TurnTokensUsed` (input/output for this specific turn)
    - `duration_ms`: `int`

### `QualitySignals`
- `finish_reason`: `str` - Why the model stopped generating (e.g., `"stop"`, `"tool_calls"`, `"length"`).
- `refusal_detected`: `bool` - `True` if the model explicitly refused a request (via OpenAI's refusal field or heuristic).
- `response_truncated`: `bool` - `True` if the response was cut off due to `max_tokens`.

---

## Example JSON Output

Here is a representation of what `agent.last_run_metadata.model_dump()` might look like after a multi-step task.

```json
{
  "request_id": "req_8f3a2b1c...",
  "timestamp": "2025-11-20T14:30:00+00:00",
  "status": "success",
  "model": {
    "provider": "openai",
    "model_id": "gpt-4o",
    "configuration": { "max_output_tokens": 4096 }
  },
  "usage": {
    "total_tokens": 1540,
    "input_tokens": 1200,
    "output_tokens": 340,
    "estimated_cost": {
      "total_usd": 0.0111,
      "input_cost_usd": 0.006,
      "output_cost_usd": 0.0051
    }
  },
  "latency": {
    "total_duration_ms": 4500,
    "time_to_first_token_ms": 850
  },
  "tools": {
    "total_tool_calls": 1,
    "tool_calls": [
      {
        "tool_call_id": "call_999",
        "tool_name": "list_directory",
        "arguments": { "path": "." },
        "execution_time_ms": 120,
        "status": "success"
      }
    ]
  },
  "turns": {
    "total_turns": 2,
    "turn_history": [
      {
        "turn_number": 1,
        "content_type": "tool_calls",
        "tools_called": ["list_directory"],
        "tokens_used": { "input_tokens": 500, "output_tokens": 40 }
      },
      {
        "turn_number": 2,
        "content_type": "text_response",
        "tools_called": [],
        "tokens_used": { "input_tokens": 700, "output_tokens": 300 }
      }
    ]
  }
}
```

Run [Metadata Example](../../examples/basic_usage.py) to see it in action!
