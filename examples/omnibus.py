"""Comprehensive test encompassing 5 providers (Groq, Mistral, Together AI, OpenAI, Anthropic).

This script demonstrates the power of the Allos SDK by chaining together multiple
providers in a single session, passing context between them, and utilizing their
unique strengths (speed, planning, coding, execution, review).
"""

import os
import time

from rich.console import Console
from rich.markdown import Markdown
from rich.panel import Panel

from allos import Agent, AgentConfig

console = Console()


def step(title):
    """Print a formatted step title to the console.

    Args:
        title (str): The title of the step to display.

    Returns:
        None
    """
    console.print(f"\n[bold blue]{'=' * 10} STEP: {title} {'=' * 10}[/]")


def run_turn(agent, prompt, expected_outcome):
    """Execute a single turn of agent interaction and display the response.

    Args:
        agent: The agent instance to run the prompt against.
        prompt (str): The input prompt to send to the agent.
        expected_outcome (str): The expected outcome or goal description for the turn.

    Returns:
        str: The response generated by the agent.
    """
    step(prompt[:50] + "...")
    console.print(f"[dim]Goal: {expected_outcome}[/dim]")

    start = time.time()
    response = agent.run(prompt)
    duration = time.time() - start

    console.print(
        Panel(
            Markdown(response),
            title=f"Agent Response ({duration:.2f}s)",
            border_style="green",
        )
    )
    return response


def get_agent_for_stage(session_file, config):
    """Creates an agent, loading context if available."""
    if os.path.exists(session_file):
        # Load previous state
        loaded_agent = Agent.load_session(session_file)
        # Create NEW agent with NEW config but OLD context
        # This is the clean way to swap providers in Python API
        return Agent(config=config, context=loaded_agent.context)
    else:
        return Agent(config=config)


def main():
    """Execute a comprehensive verification test suite for the Allos Agent SDK (Phase 2.0).

    This function orchestrates a multi-provider omnibus test that validates:
    - Provider registry and aliasing (Groq)
    - Session persistence and provider switching (Mistral)
    - Generic chat completions adapter with manual configuration (Together AI)
    - Tool execution and parameter passing (OpenAI)
    - Native provider integration and context translation (Anthropic)

    The test progresses through five stages:
    1. Groq (Llama 3.1 8b) - Tests intelligent registry and config overrides
    2. Mistral (Mistral Small) - Tests session loading and auto base-URL injection
    3. Together AI (DeepSeek Coder) - Tests explicit chat completions provider with manual base URL
    4. OpenAI (GPT-4o) - Tests tool execution (write_file, shell_exec) and token limits
    5. Anthropic (Claude 3 Haiku) - Tests native provider and context translation from OpenAI

    Each stage builds upon the previous one, passing context through saved sessions.
    Requires corresponding API keys (GROQ_API_KEY, MISTRAL_API_KEY, TOGETHER_API_KEY,
    OPENAI_API_KEY, ANTHROPIC_API_KEY) in environment variables.

    Cleans up temporary files upon completion.
    """
    console.print(
        Panel.fit(
            "[bold]Allos Omnibus Verification: Phase 2.0[/]", border_style="magenta"
        )
    )

    session_file = "omnibus_session.json"
    if os.path.exists(session_file):
        os.remove(session_file)

    # --- 1. GROQ: The Fast Ideator (Testing --no-tools and Aliasing) ---
    # We use Groq because it's fast. We disable tools to test the config override.
    # We use the alias 'groq' to test the intelligent registry.
    console.print("\n[bold cyan]--- Provider 1: Groq (Llama 3.1 8b) ---[/]")
    console.print(
        "[dim]Testing: Intelligent Registry (Alias), Config Override (no_tools)[/dim]"
    )

    if "GROQ_API_KEY" not in os.environ:
        console.print("[yellow]Skipping Groq (Key missing)[/]")
    else:
        groq_config = AgentConfig(
            provider_name="groq",
            model="llama-3.1-8b-instant",
            no_tools=True,  # Explicitly disable tools
            auto_approve=True,
        )
        agent = get_agent_for_stage(session_file, groq_config)

        run_turn(
            agent,
            "Give me ONE interesting Python automation idea involving file manipulation. Just the idea, one sentence.",
            "Fast response, no tool calls attempted.",
        )

        # Save session to hand off context
        agent.save_session(session_file)

    # --- 2. MISTRAL: The Planner (Testing Standard Compatibility) ---
    # We load the session. We switch to Mistral.
    # Mistral via API is OpenAI-compatible but uses a different endpoint.
    # Registry should handle base_url injection automatically.
    console.print("\n[bold cyan]--- Provider 2: Mistral (Mistral Small) ---[/]")
    console.print(
        "[dim]Testing: Session Loading, Provider Switching, Auto-Base-URL[/dim]"
    )

    if "MISTRAL_API_KEY" not in os.environ:
        console.print("[yellow]Skipping Mistral (Key missing)[/]")
    else:
        mistral_config = AgentConfig(
            provider_name="mistral",
            model="mistral-small-latest",
            no_tools=False,
        )
        agent = get_agent_for_stage(session_file, mistral_config)

        run_turn(
            agent,
            "Create a step-by-step implementation plan for that idea.",
            "Context aware response outlining steps.",
        )

        agent.save_session(session_file)

    # --- 3. TOGETHER AI: The Coder (Testing Explicit Chat Completions Adapter) ---
    # Here we simulate a 'custom' provider scenario by using the raw 'chat_completions' provider
    # and passing the base_url manually (even though 'together' alias exists, we want to test manual mode).
    console.print("\n[bold cyan]--- Provider 3: Together AI (DeepSeek Coder) ---[/]")
    console.print(
        "[dim]Testing: Explicit 'chat_completions' provider, Manual Base URL, API Key injection[/dim]"
    )

    if "TOGETHER_API_KEY" not in os.environ:
        console.print("[yellow]Skipping Together AI (Key missing)[/]")
    else:
        together_config = AgentConfig(
            provider_name="chat_completions",
            model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            base_url="https://api.together.xyz/v1",
            api_key=os.getenv("TOGETHER_API_KEY"),
        )
        agent = get_agent_for_stage(session_file, together_config)

        run_turn(
            agent,
            "Write the Python code for this plan. Do not execute it, just write it.",
            "Code generation using a specific model via generic adapter.",
        )

        agent.save_session(session_file)

    # --- 4. OPENAI: The Executor (Testing Tools & Max Tokens) ---
    # Finally, we hand off to GPT-4o to actually do the work.
    # We also test the 'max_tokens' parameter here just to verify it passes through.
    console.print("\n[bold cyan]--- Provider 4: OpenAI (GPT-4o) ---[/]")
    console.print(
        "[dim]Testing: Tool Execution (Write/Read/Shell), Max Tokens Param[/dim]"
    )

    if "OPENAI_API_KEY" not in os.environ:
        console.print("[yellow]Skipping OpenAI (Key missing)[/]")
    else:
        openai_config = AgentConfig(
            provider_name="openai",
            model="gpt-4o",
            tool_names=["write_file", "shell_exec"],
            max_tokens=2000,
            auto_approve=True,
        )
        agent = get_agent_for_stage(session_file, openai_config)

        # This prompt requires tools
        run_turn(
            agent,
            "Save that code to a file named 'omnibus_demo.py' and execute it. Report the output.",
            "Agent should write file, run python, and report stdout.",
        )

        agent.save_session(session_file)

    # --- 5. ANTHROPIC: The Reviewer (Testing Native Provider & Max Tokens) ---
    # We switch to Claude to review the output. This tests the Anthropic provider
    # correctly handling a context populated by OpenAI tools.
    console.print("\n[bold cyan]--- Provider 5: Anthropic (Claude 3 Haiku) ---[/]")
    console.print(
        "[dim]Testing: Native Provider, Context Translation (OpenAI -> Anthropic), Max Tokens[/dim]"
    )

    if "ANTHROPIC_API_KEY" not in os.environ:
        console.print("[yellow]Skipping Anthropic (Key missing)[/]")
    else:
        anthropic_config = AgentConfig(
            provider_name="anthropic",
            model="claude-3-haiku-20240307",
            max_tokens=1000,
        )
        agent = get_agent_for_stage(session_file, anthropic_config)

        run_turn(
            agent,
            "Analyze the output of the script execution. Did it work as expected? Be brief.",
            "Correctly interprets the previous tool output message.",
        )

    # Cleanup
    if os.path.exists("omnibus_demo.py"):
        os.remove("omnibus_demo.py")
    if os.path.exists(session_file):
        os.remove(session_file)

    console.print("\n[bold green]âœ… Omnibus Test Complete. The Allos SDK is robust.[/]")


if __name__ == "__main__":
    main()
