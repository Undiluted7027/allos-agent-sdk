"""
This example is a placeholder for a feature that is currently under development.

Feature: Ollama Provider for Local Models (Phase 2 - Post MVP)

This file will be populated with a working example demonstrating how to use
local LLMs via the Ollama provider once it is implemented.

Please see the project's roadmap for more details on our development timeline:
- MVP Roadmap: ./../MVP_ROADMAP.md
- Full Roadmap: ./../ROADMAP.md

Thank you for your interest!
"""

print(
    "This example requires the 'Ollama Provider', which is planned for a post-MVP release."
)
print("Please check back after this feature is released according to our roadmap.")

# --- Conceptual Code (will be functional in a future release) ---
#
# from allos.providers import ProviderRegistry, Message, MessageRole
#
# def run_local_model_example():
#     """
#     (Coming in a future release)
#     This function will demonstrate how to use a local model via Ollama.
#     """
#     print("\n--- Conceptual Example: Local Model with Ollama ---")
#     # Make sure your Ollama server is running
#     # ollama run llama3
#
#     # try:
#     #     local_provider = ProviderRegistry.get_provider(
#     #         "ollama",
#     #         model="llama3"
#     #     )
#     #     messages = [Message(role=MessageRole.USER, content="Why is the sky blue?")]
#     #     response = local_provider.chat(messages)
#     #     print("Response from local model:", response.content)
#     # except Exception as e:
#     #     print(f"Could not connect to local provider: {e}")
#
# if __name__ == "__main__":
#     # run_local_model_example()
#     pass
